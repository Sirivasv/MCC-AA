% Example bibtex file for TISMIR Template

@article{brown_computer_1999,
	title = {Computer identification of musical instruments using pattern recognition with cepstral coefficients as features},
	volume = {105},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.426728},
	doi = {10.1121/1.426728},
	language = {en},
	number = {3},
	urldate = {2020-02-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brown, Judith C.},
	month = mar,
	year = {1999},
	pages = {1933--1941}
}
@inproceedings{eronen_musical_2000,
	title = {Musical instrument recognition using cepstral coefficients and temporal features},
	volume = {2},
	doi = {10.1109/ICASSP.2000.859069},
	abstract = {In this paper, a system for pitch independent musical instrument recognition is presented. A wide set of features covering both spectral and temporal properties of sounds was investigated, and their extraction algorithms were designed. The usefulness of the features was validated using test data that consisted of 1498 samples covering the full pitch ranges of 30 orchestral instruments from the string, brass and woodwind families, played with different techniques. The correct instrument family was recognized with 94\% accuracy and individual instruments in 80\% of cases. These results are compared to those reported in other work. Also, utilization of a hierarchical classification framework is considered.},
	booktitle = {2000 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{00CH37100})},
	author = {Eronen, A. and Klapuri, A.},
	month = jun,
	year = {2000},
	note = {ISSN: 1520-6149},
	keywords = {feature extraction, Music, acoustic signal processing, Instruments, Algorithm design and analysis, Signal processing algorithms, brass family, cepstral analysis, Cepstral analysis, cepstral coefficients, Data mining, extraction algorithms, hierarchical classification framework, Laboratories, Multiple signal classification, musical instrument recognition, musical instruments, orchestral instrument, pattern classification, pitch independent musical instrument recognition, Signal analysis, spectral properties, string family, temporal features, temporal properties, Testing, woodwind family},
	pages = {II753--II756 vol.2}
}

@inproceedings{Hung2018FramelevelIR,
	title={Frame-level Instrument Recognition by Timbre and Pitch},
	author={Yun-Ning Hung and Yi-Hsuan Yang},
	booktitle={Proc. International Society for Music Information Retrieval Conference},
	year={2018}
}
@inproceedings{Kitahara2005InstrumentII,
	title={Instrument Identification in Polyphonic Music: Feature Weighting with Mixed Sounds, Pitch-Dependent Timbre Modeling, and Use of Musical Context},
	author={Tetsuro Kitahara and Masataka Goto and Kazunori Komatani and Tetsuya Ogata and Hiroshi G. Okuno},
	booktitle={Proc. International Society for Music Information Retrieval Conference},
	year={2005}
}
@inproceedings{Burlet2013RobotabaGT,
	title={Robotaba Guitar Tablature Transcription Framework},
	author={Gregory Burlet and Ichiro Fujinaga},
	booktitle={Proc. International Society for Music Information Retrieval Conference},
	year={2013}
}
@article{Salamon2012MelodyEF,
	title={Melody Extraction From Polyphonic Music Signals Using Pitch Contour Characteristics},
	author={Justin Salamon and Emilia G{\'o}mez},
	journal={IEEE Transactions on Audio, Speech, and Language Processing},
	year={2012},
	volume={20},
	pages={1759-1770}
}
@article{su_tent_2019,
	title = {{TENT}: {Technique}-{Embedded} {Note} {Tracking} for {Real}-{World} {Guitar} {Solo} {Recordings}},
	volume = {2},
	copyright = {Authors who publish with this journal agree to the following terms:    Authors retain copyright and grant the journal right of first publication with the work simultaneously licensed under a  Creative Commons Attribution License  that allows others to share the work with an acknowledgement of the work's authorship and initial publication in this journal.  Authors are able to enter into separate, additional contractual arrangements for the non-exclusive distribution of the journal's published version of the work (e.g., post it to an institutional repository or publish it in a book), with an acknowledgement of its initial publication in this journal.  Authors are permitted and encouraged to post their work online (e.g., in institutional repositories or on their website) prior to and during the submission process, as it can lead to productive exchanges, as well as earlier and greater citation of published work (See  The Effect of Open Access ).  All third-party images reproduced on this journal are shared under Educational Fair Use. For more information on  Educational Fair Use , please see  this useful checklist prepared by Columbia University Libraries .   All copyright  of third-party content posted here for research purposes belongs to its original owners.  Unless otherwise stated all references to characters and comic art presented on this journal are ©, ® or ™ of their respective owners. No challenge to any owner’s rights is intended or should be inferred.},
	issn = {2514-3298},
	shorttitle = {{TENT}},
	url = {http://transactions.ismir.net/articles/10.5334/tismir.23/},
	doi = {10.5334/tismir.23},
	abstract = {Article: TENT: Technique-Embedded Note Tracking for Real-World Guitar Solo Recordings},
	language = {en},
	number = {1},
	urldate = {2020-02-25},
	journal = {Transactions of the International Society for Music Information Retrieval},
	author = {Su, Ting-Wei and Chen, Yuan-Ping and Su, Li and Yang, Yi-Hsuan},
	month = jul,
	year = {2019},
	note = {Number: 1
	Publisher: Ubiquity Press},
	pages = {15--28}
}
@article{benetos_automatic_2019,
	title = {Automatic {Music} {Transcription}: {An} {Overview}},
	volume = {36},
	issn = {1558-0792},
	shorttitle = {Automatic {Music} {Transcription}},
	doi = {10.1109/MSP.2018.2869928},
	abstract = {The capability of transcribing music audio into music notation is a fascinating example of human intelligence. It involves perception (analyzing complex auditory scenes), cognition (recognizing musical objects), knowledge representation (forming musical structures), and inference (testing alternative hypotheses). Automatic music transcription (AMT), i.e., the design of computational algorithms to convert acoustic music signals into some form of music notation, is a challenging task in signal processing and artificial intelligence. It comprises several subtasks, including multipitch estimation (MPE), onset and offset detection, instrument recognition, beat and rhythm tracking, interpretation of expressive timing and dynamics, and score typesetting.},
	number = {1},
	journal = {IEEE Signal Processing Magazine},
	author = {Benetos, Emmanouil and Dixon, Simon and Duan, Zhiyao and Ewert, Sebastian},
	month = jan,
	year = {2019},
	note = {Conference Name: IEEE Signal Processing Magazine},
	keywords = {acoustic music signals, acoustic signal processing, artificial intelligence, audio signal processing, Audio systems, automatic music transcription, complex auditory scenes, computational algorithms, expressive dynamics interpretation, expressive timing interpretation, Harmonic analysis, human intelligence, instrument recognition, Instruments, knowledge representation, multipitch estimation, Multiple signal classification, music, Music, music audio transcribing, music notation, musical objects, musical structures, offset detection, onset detection, Power harmonic filters, rhythm tracking, score typesetting, signal processing, Task analysis, testing alternative hypotheses},
	pages = {20--30}
}

@inproceedings{Gururani2019AnAM,
	title={An Attention Mechanism for Musical Instrument Recognition},
	author={Siddharth Gururani and Mohit Sharma and Alexander Lerch},
	booktitle={Proc. International Society for Music Information Retrieval Conference},
	year={2019}
}
@article{benetos_automatic_2013,
	title = {Automatic music transcription: challenges and future directions},
	volume = {41},
	copyright = {2013 Springer Science+Business Media New York},
	issn = {1573-7675},
	shorttitle = {Automatic music transcription},
	url = {https://link.springer.com/article/10.1007/s10844-013-0258-3},
	doi = {10.1007/s10844-013-0258-3},
	abstract = {Automatic music transcription is considered by many to be a key enabling technology in music signal processing. However, the performance of transcription systems is still significantly below that of a human expert, and accuracies reported in recent years seem to have reached a limit, although the field is still very active. In this paper we analyse limitations of current methods and identify promising directions for future research. Current transcription methods use general purpose models which are unable to capture the rich diversity found in music signals. One way to overcome the limited performance of transcription systems is to tailor algorithms to specific use-cases. Semi-automatic approaches are another way of achieving a more reliable transcription. Also, the wealth of musical scores and corresponding audio data now available are a rich potential source of training data, via forced alignment of audio to scores, but large scale utilisation of such data has yet to be attempted. Other promising approaches include the integration of information from multiple algorithms and different musical aspects.},
	language = {en},
	number = {3},
	urldate = {2020-04-14},
	journal = {Journal of Intelligent Information Systems},
	author = {Benetos, Emmanouil and Dixon, Simon and Giannoulis, Dimitrios and Kirchhoff, Holger and Klapuri, Anssi},
	month = dec,
	year = {2013},
	note = {Company: Springer
	Distributor: Springer
	Institution: Springer
	Label: Springer
	Number: 3
	Publisher: Springer US},
	pages = {407--434}
}

@inproceedings{bogdanov_essentia_2013,
	title = {{ESSENTIA}: an {Audio} {Analysis} {Library} for {Music} {Information} {Retrieval}},
	shorttitle = {{ESSENTIA}},
	abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an ex- tensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level mu- sic descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
	author = {Bogdanov, Dmitry and Wack, N and Gómez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, G and Salamon, Justin and Zapata, Jose and Serra, Xavier},
	month = nov,
	booktitle={Proc. International Society for Music Information Retrieval Conference},
	year = {2013}
}
@InProceedings{ brian_mcfee-proc-scipy-2015,
	author    = { {M}c{F}ee, {Brian} and {R}affel, {C}olin and {L}iang, {D}awen and {D}aniel, {E}llis and {M}c{V}icar, {M}att and {B}attenberg, {E}ric and {N}ieto, {O}riol },
	title     = { librosa: {A}udio and {M}usic {S}ignal {A}nalysis in {P}ython },
	booktitle = { {P}roceedings of the 14th {P}ython in {S}cience {C}onference },
	pages     = { 18 - 24 },
	year      = { 2015 },
	editor    = { {K}athryn {H}uff and {J}ames {B}ergstra },
	doi       = { 10.25080/Majora-7b98e3ed-003 }
}

@book{gerhard_pitch_2003,
	title = {Pitch {Extraction} and {Fundamental} {Frequency}: {History} and {Current} {Techniques}},
	shorttitle = {Pitch {Extraction} and {Fundamental} {Frequency}},
	abstract = {Pitch extraction (also called fundamental frequency estimation) has been a popular topic in many fields of research since the age of computers. Yet in the course of some 50 years of study, current techniques are still not to a desired level of accuracy and robustness. When presented with a single clean pitched signal, most techniques do well, but when the signal is noisy, or when there are multiple pitch streams, many current pitch algorithms still fail to perform well. This report presents a discussion of the history of pitch detection techniques, as well as a survey of the current state of the art in pitch detection technology.},
	author = {Gerhard, David},
	month = dec,
	publisher = {Department of Computer Science, University of Regina},
	year = {2003},
	note = {Journal Abbreviation: Department of Computer Science, University of Regina, Regina, Canada
	Publication Title: Department of Computer Science, University of Regina, Regina, Canada}
}
@inproceedings{rao_pitch_2017,
	title = {Pitch prediction from {Mel}-frequency cepstral coefficients using sparse spectrum recovery},
	doi = {10.1109/NCC.2017.8077130},
	abstract = {This work proposes a technique for predicting the pitch from Mel-frequency cepstral coefficients (MFCC) vectors. Previous pitch prediction methods are based on the statistical models such as Gaussian mixture models and hidden Markov models. In this paper, we propose a three-step method to estimate pitch from MFCC vectors. First the Mel-filterbank energies (MFBEs) are estimated from MFCC vectors. Secondly, we propose a novel method to estimate the spectrum from MFBE that exploits the sparse nature of the voiced speech spectrum. Finally, the pitch is estimated from the recovered spectrum. We also explore the effect of different levels of truncation of the discrete cosine transformation (DCT) coefficients in MFCC computation on the pitch prediction error. We use the deep neutral network (DNN) based predictor as baseline to predict the pitch from MFCC vectors. The experiments using CMU-ARCTIC and KEELE database show that the proposed three-step method generalizes better across databases and genders resulting in a drop of 8Hz and 5Hz in average RMSE of predicted pitch with respect to those from DNN when 13-dimensional and 26-dimensional MFCC vectors are used for pitch prediction respectively. We also find that the sparsity constraint performs better in recovering the spectrum at lower pitch values.},
	booktitle = {2017 {Twenty}-third {National} {Conference} on {Communications} ({NCC})},
	author = {Rao, M V Achuth and Ghosh, Prasanta Kumar},
	month = mar,
	year = {2017},
	keywords = {26-dimensional MFCC vectors, cepstral analysis, channel bank filters, CMU-ARCTIC, Databases, DCT, deep neutral network based predictor, discrete cosine transformation coefficients, discrete cosine transforms, Discrete cosine transforms, DNN, Estimation, Gaussian mixture models, Gaussian processes, Harmonic analysis, hidden Markov models, Hidden Markov models, KEELE database, lower pitch values, Mel frequency cepstral coefficient, Mel-filterbank energies, Mel-frequency cepstral coefficients vectors, MFBE, MFCC computation, pitch prediction error, pitch prediction methods, predicted pitch, recovered spectrum, RMSE, sparse nature, sparse spectrum recovery, Speech, speech processing, statistical models, three-step method, vectors, voiced speech spectrum},
	pages = {1--6}
}
@article{huang_timbretron_2019,
	title = {{TimbreTron}: {A} {WaveNet}({CycleGAN}({CQT}({Audio}))) {Pipeline} for {Musical} {Timbre} {Transfer}},
	shorttitle = {{TimbreTron}},
	url = {http://arxiv.org/abs/1811.09620},
	abstract = {In this work, we address the problem of musical timbre transfer, where the goal is to manipulate the timbre of a sound sample from one instrument to match another instrument while preserving other musical content, such as pitch, rhythm, and loudness. In principle, one could apply image-based style transfer techniques to a time-frequency representation of an audio signal, but this depends on having a representation that allows independent manipulation of timbre as well as high-quality waveform generation. We introduce TimbreTron, a method for musical timbre transfer which applies "image" domain style transfer to a time-frequency representation of the audio signal, and then produces a high-quality waveform using a conditional WaveNet synthesizer. We show that the Constant Q Transform (CQT) representation is particularly well-suited to convolutional architectures due to its approximate pitch equivariance. Based on human perceptual evaluations, we confirmed that TimbreTron recognizably transferred the timbre while otherwise preserving the musical content, for both monophonic and polyphonic samples.},
	urldate = {2020-04-24},
	journal = {arXiv:1811.09620 [cs, eess, stat]},
	author = {Huang, Sicong and Li, Qiyang and Anil, Cem and Bao, Xuchan and Oore, Sageev and Grosse, Roger B.},
	month = may,
	year = {2019},
	note = {arXiv: 1811.09620},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning}
}
@inproceedings{hall_study_2012,
	address = {Berlin, Heidelberg},
	series = {Communications in {Computer} and {Information} {Science}},
	title = {Study of {Feature} {Categories} for {Musical} {Instrument} {Recognition}},
	isbn = {978-3-642-35326-0},
	doi = {10.1007/978-3-642-35326-0_16},
	abstract = {Timbre conveys powerful perceptive attribute for musical identification and has been subject of several researches. Several spectro-temporal parameters have been proposed and compared. The main research content in this paper is to examine some topic often ignored in lastly works. Several features inspired by both the psychoacoustic and perceptive knowledge’s are reexamined to explore the efficiency of the timbre dimension contribution for each feature and each category (similar processing and representation form), on a large database. Also, the impact of the normalization, time duration decomposition (short term vs. long term) and three feature selection algorithms are examined. Using the Real World Computing (RWC) music database, results shown that using 7 envelope features, the score is better than using 14 LPC coefficients and lower than using 13 MFCC coefficients. After applying Sequential Forward Selection (SFS) reduction technique to the observation vectors composed of all the 38 proposed features, 5 envelope features are retained when only 7 and 11 features are kept for MFCC and LPC, respectively.},
	language = {en},
	booktitle = {Advanced {Machine} {Learning} {Technologies} and {Applications}},
	publisher = {Springer},
	author = {Hall, Glenn Eric and Ezzaidi, Hassan and Bahoura, Mohammed},
	editor = {Hassanien, Aboul Ella and Salem, Abdel-Badeeh M. and Ramadan, Rabie and Kim, Tai-hoon},
	year = {2012},
	keywords = {features, instrument, multimedia, psychoacoustic, SFS},
	pages = {152--161}
}

@incollection{herrera-boyer_automatic_2006,
	address = {Boston, MA},
	title = {Automatic {Classification} of {Pitched} {Musical} {Instrument} {Sounds}},
	isbn = {978-0-387-32845-4},
	url = {https://doi.org/10.1007/0-387-32845-9_6},
	abstract = {This chapter discusses the problem of automatically identifying the musical instrument played in a given sound excerpt. Most of the research until now has been carried out using isolated sounds, but there is also an increasing amount of work dealing with instrument-labelling in more complex music signals, such as monotimbral phrases, duets, or even richer polyphonies. We first describe basic concepts related to acoustics, musical instruments, and perception, insofar as they are relevant for dealing, with the present problem. Then, we present a practical approach to this problem, with a special emphasis on methodological issues. Acoustic features, or, descriptors, as will be argued, are a keystone for the problem and therefore we devote a long section to some of the most useful ones, and we discuss strategies for selecting the best features when large sets of them are available. Several techniques for automatic classification, complementing those explained in Chapter 2, are described. Once the reader has been introduced to all the necessary tools, a review of the most relevant instrument classification systems is presented, including approaches that deal with continuous musical recordings. In the closing section, we summarize the main conclusions and topics for future research.},
	language = {en},
	urldate = {2020-04-24},
	booktitle = {Signal {Processing} {Methods} for {Music} {Transcription}},
	publisher = {Springer US},
	author = {Herrera-Boyer, Perfecto and Klapuri, Anssi and Davy, Manuel},
	editor = {Klapuri, Anssi and Davy, Manuel},
	year = {2006},
	doi = {10.1007/0-387-32845-9_6},
	keywords = {Automatic Classification, Linear Discriminant Analysis, Linear Prediction Coefficient, Musical Instrument, Zero Cross Rate},
	pages = {163--200}
}

@article{fuhrmann_automatic_nodate,
	title = {Automatic musical instrument recognition from polyphonic music audio signals},
	language = {en},
	author = {Fuhrmann, Ferdinand},
	pages = {265},
	journal = {Doctoral dissertation},
	publisher = {Universitat Pompeu Fabra, Spain},
	year={2012},
	file = {Fuhrmann - Automatic musical instrument recognition from poly.pdf:C\:\\Users\\sauli\\Zotero\\storage\\8KWEX97K\\Fuhrmann - Automatic musical instrument recognition from poly.pdf:application/pdf}
}
@inproceedings{muller_fmp_2019,
	address = {Delft, The Netherlands},
	title = {{FMP} {Notebooks}: {Educational} {Material} for {Teaching} and {Learning} {Fundamentals} of {Music} {Processing}},
	booktitle = {Proceedings of the {International} {Conference} on {Music} {Information} {Retrieval} ({ISMIR})},
	author = {Müller, Meinard and Zalkow, Frank},
	month = nov,
	year = {2019}
}
@inproceedings{stol_importance_2011,
	address = {Berlin, Heidelberg},
	series = {{IFIP} {Advances} in {Information} and {Communication} {Technology}},
	title = {The {Importance} of {Architectural} {Knowledge} in {Integrating} {Open} {Source} {Software}},
	isbn = {978-3-642-24418-6},
	doi = {10.1007/978-3-642-24418-6_10},
	abstract = {Open Source Software (OSS) is increasingly used in Component-Based Software Development (CBSD) of large software systems. An important issue in CBSD is selection of suitable components. Various OSS selection methods have been proposed, but most of them do not consider the software architecture aspects of OSS products. The Software Architecture (SA) research community refers to a product’s architectural information, such as design decisions and underlying rationale, and used architecture patterns, as Architecture Knowledge (AK). In order to investigate the importance of AK of OSS components in integration, we conducted an exploratory empirical study. Based on in-depth interviews with 12 IT professionals, this paper presents insights into the following questions: 1) what AK of OSS is needed? 2) Why is AK of OSS needed? 3) Is AK of OSS generally available? And 4) what is the relative importance of AK? Based on these new insights, we provide a research agenda to further the research field of software architecture in OSS.},
	language = {en},
	booktitle = {Open {Source} {Systems}: {Grounding} {Research}},
	publisher = {Springer},
	author = {Stol, Klaas-Jan and Ali Babar, Muhammad and Avgeriou, Paris},
	editor = {Hissam, Scott A. and Russo, Barbara and de Mendonça Neto, Manoel G. and Kon, Fabio},
	year = {2011},
	keywords = {architectural knowledge, component-based development, Open Source Software integration, OSS Integrator, software architecture, survey},
	pages = {142--158}
}

@article{argenti_automatic_2011,
	title = {Automatic {Transcription} of {Polyphonic} {Music} {Based} on the {Constant}-{Q} {Bispectral} {Analysis}},
	volume = {19},
	issn = {1558-7924},
	doi = {10.1109/TASL.2010.2093894},
	abstract = {In the area of music information retrieval (MIR), automatic music transcription is considered one of the most challenging tasks, for which many different techniques have been proposed. This paper presents a new method for polyphonic music transcription: a system that aims at estimating pitch, onset times, durations, and intensity of concurrent sounds in audio recordings, played by one or more instruments. Pitch estimation is carried out by means of a front-end that jointly uses a constant-Q and a bispectral analysis of the input audio signal; subsequently, the processed signal is correlated with a fixed 2-D harmonic pattern. Onsets and durations detection procedures are based on the combination of the constant-Q bispectral analysis with information from the signal spectrogram. The detection process is agnostic and it does not need to take into account musicological and instrumental models or other a priori knowledge. The system has been validated against the standard Real-World Computing (RWC)-Classical Audio Database. The proposed method has demonstrated good performances in the multiple F0 tracking task, especially for piano-only automatic transcription at MIREX 2009.},
	number = {6},
	journal = {IEEE Transactions on Audio, Speech, and Language Processing},
	author = {Argenti, Fabrizio and Nesi, Paolo and Pantaleo, Gianni},
	month = aug,
	year = {2011},
	note = {Conference Name: IEEE Transactions on Audio, Speech, and Language Processing},
	keywords = {2D harmonic pattern, audio recordings, audio signal processing, Audio signals processing, automatic music transcription, bispectrum, constant-Q analysis, constant-Q bispectral analysis, duration detection, Estimation, estimation theory, Filter bank, Fourier transforms, Harmonic analysis, Hidden Markov models, higher order spectra, information retrieval, MIR, music, music information retrieval, music information retrieval (MIR), onset detection, pitch estimation, polyphonic music transcription, Psychoacoustic models, real-world computing classical audio database, RWC-classical audio database, signal spectrogram, Spectral analysis},
	pages = {1610--1630}
}
@article{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2020-04-28},
	journal = {arXiv:1704.01279 [cs]},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.01279},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound}
}

@article{schoerkhuber_constant-q_2010,
	title = {Constant-{Q} transform toolbox for music processing},
	journal = {7th Sound and Music Computing Conference},
	author = {Schoerkhuber, Christian and Anssi, Klapuri},
	year = {2010}
}
@article{drugman_traditional_2018,
	title = {Traditional {Machine} {Learning} for {Pitch} {Detection}},
	volume = {25},
	issn = {1558-2361},
	doi = {10.1109/LSP.2018.2874155},
	abstract = {Pitch detection is a fundamental problem in speech processing as F0 is used in a large number of applications. Recent papers have proposed deep learning for robust pitch tracking. In this letter, we consider voicing detection as a classification problem and F0 contour estimation as a regression problem. For both tasks, acoustic features from multiple domains and traditional machine learning methods are used. The discrimination power of existing and proposed features is assessed through mutual information. Multiple supervised and unsupervised approaches are compared. A significant relative reduction of voicing errors over the best baseline is obtained-20\% with the best clustering method (K-means) and 45\% with a multi-layer perceptron. For F0 contour estimation, the benefits of regression techniques are limited though. We investigate whether those objective gains translate in a parametric synthesis task. Clear perceptual preferences are observed for the proposed approach over two widely used baselines (robust algorithm for pitch tracking (RAPT) and distributed inline-filter operation (DIO)).},
	number = {11},
	journal = {IEEE Signal Processing Letters},
	author = {Drugman, Thomas and Huybrechts, Goeric and Klimkov, Viacheslav and Moinet, Alexis},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Signal Processing Letters},
	keywords = {Machine learning, feature extraction, learning (artificial intelligence), Task analysis, acoustic signal processing, Training, deep learning, signal classification, estimation theory, pitch tracking, Signal processing algorithms, speech processing, Cepstral analysis, acoustic features, classification problem, clustering method, discrimination power, F0 contour estimation, Feature extraction, Fundamental frequency, fundamental problem, machine learning methods, multilayer perceptron, multilayer perceptrons, parametric synthesis task, pitch detection, regression analysis, regression problem, regression techniques, relative reduction, robust pitch tracking, speech synthesis, voicing decision},
	pages = {1745--1749}
}

@book{schnupp_auditory_2011,
	address = {Cambridge, MA, US},
	series = {Auditory neuroscience: {Making} sense of sound},
	title = {Auditory neuroscience: {Making} sense of sound},
	isbn = {978-0-262-11318-2},
	shorttitle = {Auditory neuroscience},
	abstract = {In Auditory Neuroscience: Making Sense of Sound, we are trying to explain auditory perception in terms of the neural processes that take place in different parts of the auditory system. In doing so, we present selected highlights from a very long and large research project: It started more than 400 years ago and it may not be completed for another 400 years. As you will see, some of the questions we raised above can already be answered very clearly, while for others our answers are still tentative, with many important details unresolved. Neurophysiologists are not yet in a position to give a complete account of how the stream of numbers in the digital audio player is turned into the experience of music. Nevertheless, progress in this area is rapid, and many of the deep questions of auditory perception are being addressed today in terms of the responses of nerve cells and the brain circuits they make up. These are exciting times for auditory neuroscientists, and we hope that at least some of our readers will be inspired by this book to join the auditory neuroscience community and help complete the picture that is currently emerging on physical acoustics and the physiology of the ear. The book is divided into eight chapters. The first two provide essential background on physical acoustics and the physiology of the ear. In the chapters that follow, we have consciously avoided trying to "work our way up the ascending auditory pathway" structure by structure. Instead, in chapters 3 to 6, we explore the neurobiology behind four aspects of hearing—namely, the perception of pitch, the processing of speech, the localization of sound sources, and the perceptual separation of sound mixtures. The final two chapters delve into the development and plasticity of the auditory system, and briefly discuss contemporary technologies aimed at treating hearing loss, such as hearing aids and cochlear implants. (PsycINFO Database Record (c) 2016 APA, all rights reserved)},
	publisher = {MIT Press},
	author = {Schnupp, Jan and Nelken, Israel and King, Andrew},
	year = {2011},
	note = {Pages: x, 356},
	keywords = {Auditory Perception, Auditory Stimulation, Cochlear Implants, Hearing Disorders, Neural Plasticity, Neurobiology, Neurosciences, Speech Perception}
}


@book{siedenburg_timbre_2019,
	series = {Springer {Handbook} of {Auditory} {Research}},
	title = {Timbre: {Acoustics}, {Perception}, and {Cognition}},
	isbn = {978-3-030-14831-7},
	shorttitle = {Timbre},
	url = {https://www.springer.com/gp/book/9783030148317},
	abstract = {Roughly defined as any property other than pitch, duration, and loudness that allows two sounds to be distinguished, timbre is a foundational aspect of hearing. The remarkable ability of humans to recognize sound sources and events (e.g., glass breaking, a friend’s voice, a tone from a piano) stems primarily from a capacity to perceive and process differences in the timbre of sounds. Timbre raises many important issues in psychology and the cognitive sciences, musical acoustics, speech processing, medical engineering, and artificial intelligence. Current research on timbre perception unfolds along three main fronts: On the one hand, researchers explore the principal perceptual processes that orchestrate timbre processing, such as the structure of its perceptual representation, sound categorization and recognition, memory for timbre, and its ability to elicit rich semantic associations, as well as the underlying neural mechanisms. On the other hand, timbre is studied as part of specific scenarios, including the perception of the human voice, as a structuring force in music, as perceived with cochlear implants, and through its role in affecting sound quality and sound design. Finally, computational acoustic models are sought through prediction of psychophysical data, physiologically inspired representations, and audio analysis-synthesis techniques. Along these three scientific fronts, significant breakthroughs have been achieved during the last decade. This volume will be the first book dedicated to a comprehensive and authoritative presentation of timbre perception and cognition research and the acoustic modeling of timbre. The volume will serve as a natural complement to the SHAR volumes on the basic auditory parameters of Pitch edited by Plack, Oxenham, Popper, and Fay, and Loudness by Florentine, Popper, and Fay. Moreover, through the integration of complementary scientific methods ranging from signal processing to brain imaging, the book has the potential to leverage new interdisciplinary synergies in hearing science. For these reasons, the volume will be exceptionally valuable to various subfields of hearing science, including cognitive auditory neuroscience, psychoacoustics, music perception and cognition, but may even exert significant influence on fields such as musical acoustics, music information retrieval, and acoustic signal processing.It is expected that the volume will have broad appeal to psychologists, neuroscientists, and acousticians involved in research on auditory perception and cognition. Specifically, this book will have a strong impact on hearing researchers with interest in timbre and will serve as the key publication and up-to-date reference on timbre for graduate students, postdoctoral researchers, as well as established scholars.},
	language = {en},
	urldate = {2020-06-15},
	publisher = {Springer International Publishing},
	editor = {Siedenburg, Kai and Saitis, Charalampos and McAdams, Stephen and Popper, Arthur N. and Fay, Richard R.},
	year = {2019},
	doi = {10.1007/978-3-030-14832-4}
}
@article{brown_calculation_1991,
	title = {Calculation of a constant {Q} spectral transform},
	volume = {89},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.400476},
	doi = {10.1121/1.400476},
	language = {en},
	number = {1},
	urldate = {2020-02-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brown, Judith C.},
	month = jan,
	year = {1991},
	pages = {425--434}
}

