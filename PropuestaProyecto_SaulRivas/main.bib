
@article{hung_frame-level_nodate,
	title = {{FRAME}-{LEVEL} {INSTRUMENT} {RECOGNITION} {BY} {TIMBRE} {AND} {PITCH}},
	abstract = {Instrument recognition is a fundamental task in music information retrieval, yet little has been done to predict the presence of instruments in multi-instrument music for each time frame. This task is important for not only automatic transcription but also many retrieval problems. In this paper, we use the newly released MusicNet dataset to study this front, by building and evaluating a convolutional neural network for making frame-level instrument prediction. We consider it as a multi-label classiﬁcation problem for each frame and use frame-level annotations as the supervisory signal in training the network. Moreover, we experiment with different ways to incorporate pitch information to our model, with the premise that doing so informs the model the notes that are active per frame, and also encourages the model to learn relative rates of energy buildup in the harmonic partials of different instruments. Experiments show salient performance improvement over baseline methods. We also report an analysis probing how pitch information helps the instrument prediction task. Code and experiment details can be found at https://biboamy. github.io/instrument-recognition/.},
	language = {en},
	author = {Hung, Yun-Ning and Yang, Yi-Hsuan},
	pages = {8},
	file = {Hung and Yang - FRAME-LEVEL INSTRUMENT RECOGNITION BY TIMBRE AND P.pdf:/home/sirivasv/Zotero/storage/3ELME8P5/Hung and Yang - FRAME-LEVEL INSTRUMENT RECOGNITION BY TIMBRE AND P.pdf:application/pdf;Hung and Yang - FRAME-LEVEL INSTRUMENT RECOGNITION BY TIMBRE AND P.pdf:/home/sirivasv/Documents/researchlibrary/Hung and Yang - FRAME-LEVEL INSTRUMENT RECOGNITION BY TIMBRE AND P.pdf:application/pdf}
}

@article{kitahara_instrument_nodate,
	title = {Instrument {Identification} in {Polyphonic} {Music}: {Feature} {Weighting} with {Mixed} {Sounds}, {Pitch}-{Dependent} {Timbre} {Modeling}, and {Use} of {Musical} {Context}},
	abstract = {This paper addresses the problem of identifying musical instruments in polyphonic music. Musical instrument identiﬁcation (MII) is an improtant task in music information retrieval because MII results make it possible to automatically retrieving certain types of music (e.g., piano sonata, string quartet). Only a few studies, however, have dealt with MII in polyphonic music. In MII in polyphonic music, there are three issues: feature variations caused by sound mixtures, the pitch dependency of timbres, and the use of musical context. For the ﬁrst issue, templates of feature vectors representing timbres are extracted from not only isolated sounds but also sound mixtures. Because some features are not robust in the mixtures, features are weighted according to their robustness by using linear discriminant analysis. For the second issue, we use an F0-dependent multivariate normal distribution, which approximates the pitch dependency as a function of fundamental frequency. For the third issue, when the instrument of each note is identiﬁed, the a priori probablity of the note is calculated from the a posteriori probabilities of temporally neighboring notes. Experimental results showed that recognition rates were improved from 60.8\% to 85.8\% for trio music and from 65.5\% to 91.1\% for duo music.},
	language = {en},
	author = {Kitahara, Tetsuro},
	pages = {6},
	file = {Kitahara - Instrument Identification in Polyphonic Music Fea.pdf:/home/sirivasv/Zotero/storage/XTVQGHHU/Kitahara - Instrument Identification in Polyphonic Music Fea.pdf:application/pdf;Kitahara - Instrument Identification in Polyphonic Music Fea.pdf:/home/sirivasv/Documents/researchlibrary/Kitahara - Instrument Identification in Polyphonic Music Fea.pdf:application/pdf}
}

@inproceedings{eronen_musical_2000,
	title = {Musical instrument recognition using cepstral coefficients and temporal features},
	volume = {2},
	doi = {10.1109/ICASSP.2000.859069},
	abstract = {In this paper, a system for pitch independent musical instrument recognition is presented. A wide set of features covering both spectral and temporal properties of sounds was investigated, and their extraction algorithms were designed. The usefulness of the features was validated using test data that consisted of 1498 samples covering the full pitch ranges of 30 orchestral instruments from the string, brass and woodwind families, played with different techniques. The correct instrument family was recognized with 94\% accuracy and individual instruments in 80\% of cases. These results are compared to those reported in other work. Also, utilization of a hierarchical classification framework is considered.},
	booktitle = {2000 {IEEE} {International} {Conference} on {Acoustics}, {Speech}, and {Signal} {Processing}. {Proceedings} ({Cat}. {No}.{00CH37100})},
	author = {Eronen, A. and Klapuri, A.},
	month = jun,
	year = {2000},
	note = {ISSN: 1520-6149},
	keywords = {feature extraction, Music, acoustic signal processing, Instruments, Algorithm design and analysis, Signal processing algorithms, brass family, cepstral analysis, Cepstral analysis, cepstral coefficients, Data mining, extraction algorithms, hierarchical classification framework, Laboratories, Multiple signal classification, musical instrument recognition, musical instruments, orchestral instrument, pattern classification, pitch independent musical instrument recognition, Signal analysis, spectral properties, string family, temporal features, temporal properties, Testing, woodwind family},
	pages = {II753--II756 vol.2},
	file = {IEEE Xplore Full Text PDF:/home/sirivasv/Zotero/storage/GTYIUBYL/Eronen and Klapuri - 2000 - Musical instrument recognition using cepstral coef.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sirivasv/Zotero/storage/Q2BKNFNC/859069.html:text/html;Eronen and Klapuri - 2000 - Musical instrument recognition using cepstral coef.pdf:/home/sirivasv/Documents/researchlibrary/Eronen and Klapuri - 2000 - Musical instrument recognition using cepstral coef.pdf:application/pdf}
}

@article{brown_computer_1999,
	title = {Computer identification of musical instruments using pattern recognition with cepstral coefficients as features},
	volume = {105},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.426728},
	doi = {10.1121/1.426728},
	language = {en},
	number = {3},
	urldate = {2020-02-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brown, Judith C.},
	month = mar,
	year = {1999},
	pages = {1933--1941},
	file = {Brown - 1999 - Computer identification of musical instruments usi.pdf:/home/sirivasv/Zotero/storage/3Y35BM9H/Brown - 1999 - Computer identification of musical instruments usi.pdf:application/pdf;Brown - 1999 - Computer identification of musical instruments usi.pdf:/home/sirivasv/Documents/researchlibrary/Brown - 1999 - Computer identification of musical instruments usi.pdf:application/pdf}
}

@article{brown_calculation_1991,
	title = {Calculation of a constant {Q} spectral transform},
	volume = {89},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.400476},
	doi = {10.1121/1.400476},
	language = {en},
	number = {1},
	urldate = {2020-02-23},
	journal = {The Journal of the Acoustical Society of America},
	author = {Brown, Judith C.},
	month = jan,
	year = {1991},
	pages = {425--434},
	file = {Brown - 1991 - Calculation of a constant iQi spectral transf.pdf:/home/sirivasv/Zotero/storage/J69CFWU6/Brown - 1991 - Calculation of a constant iQi spectral transf.pdf:application/pdf;Brown - 1991 - Calculation of a constant iQi spectral transf.pdf:/home/sirivasv/Documents/researchlibrary/Brown - 1991 - Calculation of a constant iQi spectral transf.pdf:application/pdf}
}

@incollection{oshaughnessy_speech_2000,
	title = {Speech {Analysis}},
	isbn = {null},
	url = {https://ieeexplore-ieee-org.pbidi.unam.mx:2443/document/5312308},
	booktitle = {Speech {Communications}: {Human} and {Machine}},
	publisher = {IEEE},
	author = {O'Shaughnessy, D.},
	year = {2000},
	doi = {10.1109/9780470546475.ch6},
	pages = {173--227},
	file = {5312308.pdf:/home/sirivasv/Documents/researchlibrary/5312308.pdf:application/pdf}
}

@article{popat_cluster-based_1997,
	title = {Cluster-based probability model and its application to image and texture processing},
	volume = {6},
	issn = {1941-0042},
	doi = {10.1109/83.551697},
	abstract = {We develop, analyze, and apply a specific form of mixture modeling for density estimation within the context of image and texture processing. The technique captures much of the higher order, nonlinear statistical relationships present among vector elements by combining aspects of kernel estimation and cluster analysis. Experimental results are presented in the following applications: image restoration, image and texture compression, and texture classification.},
	number = {2},
	journal = {IEEE Transactions on Image Processing},
	author = {Popat, K. and Picard, R.W.},
	month = feb,
	year = {1997},
	keywords = {cluster analysis, cluster-based probability model, Context modeling, data compression, density estimation, Extraterrestrial phenomena, higher order nonlinear statistical relationships, higher order statistics, Image analysis, image classification, image coding, Image coding, image compression, image processing, image restoration, Image restoration, image texture, Image texture analysis, Kernel, kernel estimation, mixture modeling, parameter estimation, probability, Probability distribution, Signal processing, Tail, texture classification, texture compression, texture processing, vector elements},
	pages = {268--284},
	file = {IEEE Xplore Abstract Record:/home/sirivasv/Zotero/storage/Q7CELHP7/551697.html:text/html;Submitted Version:/home/sirivasv/Zotero/storage/AII3GSZC/Popat and Picard - 1997 - Cluster-based probability model and its applicatio.pdf:application/pdf;Popat and Picard - 1997 - Cluster-based probability model and its applicatio.pdf:/home/sirivasv/Documents/researchlibrary/Popat and Picard - 1997 - Cluster-based probability model and its applicatio.pdf:application/pdf}
}

@inproceedings{mauch_pyin_2014,
	title = {{PYIN}: {A} fundamental frequency estimator using probabilistic threshold distributions},
	shorttitle = {{PYIN}},
	doi = {10.1109/ICASSP.2014.6853678},
	abstract = {We propose the Probabilistic YIN (PYIN) algorithm, a modification of the well-known YIN algorithm for fundamental frequency (F0) estimation. Conventional YIN is a simple yet effective method for frame-wise monophonic F0 estimation and remains one of the most popular methods in this domain. In order to eliminate short-term errors, outputs of frequency estimators are usually post-processed resulting in a smoother pitch track. One shortcoming of YIN is that such post-processing cannot fall back on alternative interpretations of the signal because the method outputs precisely one estimate per frame. To address this problem we modify YIN to output multiple pitch candidates with associated probabilities (PYIN Stage 1). These probabilities arise naturally from a prior distribution on the YIN threshold parameter. We use these probabilities as observations in a hidden Markov model, which is Viterbi-decoded to produce an improved pitch track (PYIN Stage 2). We demonstrate that the combination of Stages 1 and 2 raises recall and precision substantially. The additional computational complexity of PYIN over YIN is low. We make the method freely available online1 as an open source C++ library for Vamp hosts.},
	booktitle = {2014 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing} ({ICASSP})},
	author = {Mauch, Matthias and Dixon, Simon},
	month = may,
	year = {2014},
	note = {ISSN: 2379-190X},
	keywords = {Algorithm design and analysis, computational complexity, Databases, framewise monophonic F0 estimation, frequency estimation, Frequency estimation, fundamental frequency estimation, fundamental frequency estimators, hidden Markov model, hidden Markov models, Hidden Markov models, open source C++ library, Pitch estimation, pitch track, pitch tracking, post-processing, Probabilistic logic, probabilistic threshold distributions, probabilistic YIN algorithm, PYIN algorithm, short-term errors, Signal processing algorithms, Smoothing methods, speech processing, statistical distributions, Vamp hosts, Viterbi, Viterbi decoding, YIN, YIN threshold parameter},
	pages = {659--663},
	file = {IEEE Xplore Full Text PDF:/home/sirivasv/Zotero/storage/SWGCLFEJ/Mauch and Dixon - 2014 - PYIN A fundamental frequency estimator using prob.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sirivasv/Zotero/storage/K587KKPR/6853678.html:text/html;06853678.pdf:/home/sirivasv/Documents/researchlibrary/06853678.pdf:application/pdf}
}

@article{de_cheveigne_yin_2002,
	title = {{YIN}, a fundamental frequency estimator for speech and music},
	volume = {111},
	issn = {0001-4966},
	url = {https://asa.scitation.org/doi/10.1121/1.1458024},
	doi = {10.1121/1.1458024},
	number = {4},
	urldate = {2020-02-27},
	journal = {The Journal of the Acoustical Society of America},
	author = {de Cheveigné, Alain and Kawahara, Hideki},
	month = apr,
	year = {2002},
	note = {Publisher: Acoustical Society of America},
	pages = {1917--1930},
	file = {Snapshot:/home/sirivasv/Zotero/storage/PNZVULYV/1.html:text/html;Full Text PDF:/home/sirivasv/Zotero/storage/TTQKQM34/de Cheveigné y Kawahara - 2002 - YIN, a fundamental frequency estimator for speech .pdf:application/pdf}
}

@article{kim_crepe_2018,
	title = {{CREPE}: {A} {Convolutional} {Representation} for {Pitch} {Estimation}},
	shorttitle = {{CREPE}},
	url = {http://arxiv.org/abs/1802.06182},
	abstract = {The task of estimating the fundamental frequency of a monophonic sound recording, also known as pitch tracking, is fundamental to audio processing with multiple applications in speech processing and music information retrieval. To date, the best performing techniques, such as the pYIN algorithm, are based on a combination of DSP pipelines and heuristics. While such techniques perform very well on average, there remain many cases in which they fail to correctly estimate the pitch. In this paper, we propose a data-driven pitch tracking algorithm, CREPE, which is based on a deep convolutional neural network that operates directly on the time-domain waveform. We show that the proposed model produces state-of-the-art results, performing equally or better than pYIN. Furthermore, we evaluate the model's generalizability in terms of noise robustness. A pre-trained version of CREPE is made freely available as an open-source Python module for easy application.},
	urldate = {2020-02-27},
	journal = {arXiv:1802.06182 [cs, eess, stat]},
	author = {Kim, Jong Wook and Salamon, Justin and Li, Peter and Bello, Juan Pablo},
	month = feb,
	year = {2018},
	note = {arXiv: 1802.06182},
	keywords = {Computer Science - Machine Learning, Computer Science - Sound, Electrical Engineering and Systems Science - Audio and Speech Processing, Statistics - Machine Learning},
	file = {arXiv.org Snapshot:/home/sirivasv/Zotero/storage/7UDHKVU6/1802.html:text/html;arXiv Fulltext PDF:/home/sirivasv/Zotero/storage/8KK5ZKIA/Kim et al. - 2018 - CREPE A Convolutional Representation for Pitch Es.pdf:application/pdf}
}

@article{drugman_traditional_2018,
	title = {Traditional {Machine} {Learning} for {Pitch} {Detection}},
	volume = {25},
	issn = {1558-2361},
	doi = {10.1109/LSP.2018.2874155},
	abstract = {Pitch detection is a fundamental problem in speech processing as F0 is used in a large number of applications. Recent papers have proposed deep learning for robust pitch tracking. In this letter, we consider voicing detection as a classification problem and F0 contour estimation as a regression problem. For both tasks, acoustic features from multiple domains and traditional machine learning methods are used. The discrimination power of existing and proposed features is assessed through mutual information. Multiple supervised and unsupervised approaches are compared. A significant relative reduction of voicing errors over the best baseline is obtained-20\% with the best clustering method (K-means) and 45\% with a multi-layer perceptron. For F0 contour estimation, the benefits of regression techniques are limited though. We investigate whether those objective gains translate in a parametric synthesis task. Clear perceptual preferences are observed for the proposed approach over two widely used baselines (robust algorithm for pitch tracking (RAPT) and distributed inline-filter operation (DIO)).},
	number = {11},
	journal = {IEEE Signal Processing Letters},
	author = {Drugman, Thomas and Huybrechts, Goeric and Klimkov, Viacheslav and Moinet, Alexis},
	month = nov,
	year = {2018},
	note = {Conference Name: IEEE Signal Processing Letters},
	keywords = {acoustic features, acoustic signal processing, Cepstral analysis, classification problem, clustering method, deep learning, discrimination power, estimation theory, F0 contour estimation, feature extraction, Feature extraction, Fundamental frequency, fundamental problem, learning (artificial intelligence), Machine learning, machine learning methods, multilayer perceptron, multilayer perceptrons, parametric synthesis task, pitch detection, pitch tracking, regression analysis, regression problem, regression techniques, relative reduction, robust pitch tracking, signal classification, Signal processing algorithms, speech processing, speech synthesis, Task analysis, Training, voicing decision},
	pages = {1745--1749},
	file = {Versión enviada:/home/sirivasv/Zotero/storage/PFNEBRIR/Drugman et al. - 2018 - Traditional Machine Learning for Pitch Detection.pdf:application/pdf;IEEE Xplore Abstract Record:/home/sirivasv/Zotero/storage/LKEJ56WZ/8481420.html:text/html}
}

@article{mcvicar_machine_nodate,
	title = {A {Machine} {Learning} {Approach} to {Automatic} {Chord} {Estimation}},
	url = {https://www.academia.edu/18861669/A_Machine_Learning_Approach_to_Automatic_Chord_Estimation},
	abstract = {In this thesis we introduce a machine learning based automatic chord recognition algorithm that achieves state of the art performance. This performance is realised by the introduction of a novel Dynamic Bayesian Net- work and chromagram feature},
	language = {en},
	urldate = {2020-02-27},
	author = {McVicar, Matt},
	file = {Snapshot:/home/sirivasv/Zotero/storage/JXWVW3TR/A_Machine_Learning_Approach_to_Automatic_Chord_Estimation.html:text/html}
}

@article{engel_neural_2017,
	title = {Neural {Audio} {Synthesis} of {Musical} {Notes} with {WaveNet} {Autoencoders}},
	url = {http://arxiv.org/abs/1704.01279},
	abstract = {Generative models in vision have seen rapid progress due to algorithmic improvements and the availability of high-quality image datasets. In this paper, we offer contributions in both these areas to enable similar progress in audio modeling. First, we detail a powerful new WaveNet-style autoencoder model that conditions an autoregressive decoder on temporal codes learned from the raw audio waveform. Second, we introduce NSynth, a large-scale and high-quality dataset of musical notes that is an order of magnitude larger than comparable public datasets. Using NSynth, we demonstrate improved qualitative and quantitative performance of the WaveNet autoencoder over a well-tuned spectral autoencoder baseline. Finally, we show that the model learns a manifold of embeddings that allows for morphing between instruments, meaningfully interpolating in timbre to create new types of sounds that are realistic and expressive.},
	urldate = {2020-02-28},
	journal = {arXiv:1704.01279 [cs]},
	author = {Engel, Jesse and Resnick, Cinjon and Roberts, Adam and Dieleman, Sander and Eck, Douglas and Simonyan, Karen and Norouzi, Mohammad},
	month = apr,
	year = {2017},
	note = {arXiv: 1704.01279},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Machine Learning, Computer Science - Sound},
	file = {arXiv Fulltext PDF:/home/sirivasv/Zotero/storage/5GFAFVMZ/Engel et al. - 2017 - Neural Audio Synthesis of Musical Notes with WaveN.pdf:application/pdf;arXiv.org Snapshot:/home/sirivasv/Zotero/storage/BJUD2GFH/1704.html:text/html}
}

@article{mcfee_librosa_2015,
	title = {librosa: {Audio} and {Music} {Signal} {Analysis} in {Python}},
	shorttitle = {librosa},
	url = {https://scinapse.io/papers/2191779130},
	doi = {10.25080/majora-7b98e3ed-003},
	abstract = {This document describes version 0.4.0 of librosa: a Python pack- age for audio and music signal processing. At {\textbar} Brian McFee, Colin Raffel, Dawen Liang, Daniel P.   {\textbar}},
	language = {en},
	urldate = {2020-02-28},
	author = {McFee, Brian and Raffel, Colin and Liang, Dawen and Ellis, Daniel P. W. and McVicar, Matt and Battenberg, Eric and Nieto, Oriol},
	month = jan,
	year = {2015},
	file = {Snapshot:/home/sirivasv/Zotero/storage/4FXR678X/2191779130.html:text/html;Full Text:/home/sirivasv/Zotero/storage/HJ5GTEBE/McFee et al. - 2015 - librosa Audio and Music Signal Analysis in Python.pdf:application/pdf}
}

@article{das_real-time_2017,
	title = {Real-time {Pitch} {Tracking} in {Audio} {Signals} with the {Extended} {Complex} {Kalman} {Filter}},
	abstract = {The Kalman ﬁlter is a well-known tool used extensively in robotics, navigation, speech enhancement and ﬁnance. In this paper, we propose a novel pitch follower based on the Extended Complex Kalman Filter (ECKF). An advantage of this pitch follower is that it operates on a sample-by-sample basis, unlike other block-based algorithms that are most commonly used in pitch estimation. Thus, it estimates sample-synchronous fundamental frequency (assumed to be the perceived pitch), which makes it ideal for real-time implementation. Simultaneously, the ECKF also tracks the amplitude envelope of the input audio signal. Finally, we test our ECKF pitch detector on a number of cello and double bass recordings played with various ornaments, such as vibrato, portamento and trill, and compare its result with the well-known YIN estimator, to conclude the effectiveness of our algorithm.},
	language = {en},
	author = {Das, Orchisama and Iii, Julius O Smith and Chafe, Chris},
	year = {2017},
	pages = {7},
	file = {Das et al. - 2017 - Real-time Pitch Tracking in Audio Signals with the.pdf:/home/sirivasv/Zotero/storage/S5X94IWQ/Das et al. - 2017 - Real-time Pitch Tracking in Audio Signals with the.pdf:application/pdf}
}

@inproceedings{bogdanov_essentia_2013,
	title = {{ESSENTIA}: an {Audio} {Analysis} {Library} for {Music} {Information} {Retrieval}},
	shorttitle = {{ESSENTIA}},
	abstract = {We present Essentia 2.0, an open-source C++ library for audio analysis and audio-based music information retrieval released under the Affero GPL license. It contains an ex- tensive collection of reusable algorithms which implement audio input/output functionality, standard digital signal processing blocks, statistical characterization of data, and a large set of spectral, temporal, tonal and high-level mu- sic descriptors. The library is also wrapped in Python and includes a number of predefined executable extractors for the available music descriptors, which facilitates its use for fast prototyping and allows setting up research experiments very rapidly. Furthermore, it includes a Vamp plugin to be used with Sonic Visualiser for visualization purposes. The library is cross-platform and currently supports Linux, Mac OS X, and Windows systems. Essentia is designed with a focus on the robustness of the provided music descriptors and is optimized in terms of the computational cost of the algorithms. The provided functionality, specifically the music descriptors included in-the-box and signal processing algorithms, is easily expandable and allows for both research experiments and development of large-scale industrial applications.},
	author = {Bogdanov, Dmitry and Wack, N and Gómez, Emilia and Gulati, Sankalp and Herrera, Perfecto and Mayor, Oscar and Roma, G and Salamon, Justin and Zapata, Jose and Serra, Xavier},
	month = nov,
	year = {2013},
	file = {Full Text PDF:/home/sirivasv/Zotero/storage/24PHECFG/Bogdanov et al. - 2013 - ESSENTIA an Audio Analysis Library for Music Info.pdf:application/pdf}
}

@article{brown_efficient_1992,
	title = {An efficient algorithm for the calculation of a constant {Q} transform},
	volume = {92},
	doi = {10.1121/1.404385},
	abstract = {An efficient method of transforming a discrete Fourier transform (DFT) into a constant Q transform, where Q is the ratio of center frequency to bandwidth, has been devised. This method involves the calculation of kernels that are then applied to each subsequent DFT. Only a few multiples are involved in the calculation of each component of the constant Q transform, so this transformation adds a small amount to the computation. In effect, this method makes it possible to take full advantage of the computational efficiency of the fast Fourier transform (FFT). Graphical examples of the application of this calculation to musical signals are given for sounds produced by a clarinet and a violin.},
	journal = {Journal of the Acoustical Society of America},
	author = {Brown, Judith and Puckette, Miller},
	month = nov,
	year = {1992},
	pages = {2698},
	file = {Full Text PDF:/home/sirivasv/Zotero/storage/47H3H6BS/Brown and Puckette - 1992 - An efficient algorithm for the calculation of a c.pdf:application/pdf}
}